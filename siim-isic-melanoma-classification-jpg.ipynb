{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\ono008\\.conda\\envs\\oystein_tf2_gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\oystein_tf2_gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m   \u001b[1;31m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c79c681ef33f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mKL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\oystein_tf2_gpu\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\oystein_tf2_gpu\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\oystein_tf2_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrewriter_config_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\oystein_tf2_gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tfe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\oystein_tf2_gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 83\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\ono008\\.conda\\envs\\oystein_tf2_gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import shutil\n",
    "import datetime\n",
    "import geopandas\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as KL\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Check GPUs:\",\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            # Prevent TensorFlow from allocating all memory of all GPUs:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "#policy = mixed_precision.Policy('mixed_float16')\n",
    "#mixed_precision.set_policy(policy)\n",
    "#print('Compute dtype: %s' % policy.compute_dtype)\n",
    "#print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JPEG_TRAIN = 'data/siim-isic-melanoma-classification/jpeg/train'\n",
    "JPEG_VAL = 'data/siim-isic-melanoma-classification/jpeg/val'\n",
    "JPEG_TEST = 'data/siim-isic-melanoma-classification/jpeg/test'\n",
    "CSV_TRAIN = 'data/siim-isic-melanoma-classification/train.csv'\n",
    "CSV_TEST = 'data/siim-isic-melanoma-classification/test.csv'\n",
    "SUBMITS_DIR = 'submits/siim-isic-melanoma-classification'\n",
    "\n",
    "#WIDTH, HEIGHT = 224, 224\n",
    "WIDTH, HEIGHT = 448, 448\n",
    "WIDTH, HEIGHT = 896, 896\n",
    "WIDTH, HEIGHT = 768, 768\n",
    "\n",
    "#WIDTH, HEIGHT = 1120, 1120\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(CSV_TRAIN)\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(meta_df, train_path, val_path, val_percent = 0.25, seed = 18):\n",
    "    n = len(meta_df)\n",
    "    n_val = int(n * val_percent)\n",
    "    n_train = n - n_val\n",
    "    train_val = ['train'] * n_train + ['val'] * n_val\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_val)\n",
    "    meta_df['train_val'] = train_val\n",
    "    \n",
    "    p = pathlib.Path(train_path)\n",
    "    l = list(p.glob('**/*.jpg'))\n",
    "    if len(l) != n:\n",
    "        print('Is train/val split already done?')\n",
    "        return meta_df\n",
    "    \n",
    "    i = 0\n",
    "    for source in l:\n",
    "        #print(source)\n",
    "        if meta_df.loc[meta_df['image_name'] == source.stem, 'train_val'].values[0] == 'val':\n",
    "            #print('val', source.stem)\n",
    "            dest = pathlib.Path(val_path, source.name)\n",
    "            #print(dest)\n",
    "            shutil.move(source, dest)\n",
    "            i += 1\n",
    "    \n",
    "    if i == n_val:\n",
    "        print(n_val, 'validation images moved to validation directory')\n",
    "    else:\n",
    "        print('There is a discrepancy in number of validation images moved')\n",
    "        print('Images supposed to be moved:', n_val)\n",
    "        print('Images moved:', i)\n",
    "    \n",
    "    return meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_target_subfolders(meta_df, jpeg_path, target_list):\n",
    "    \n",
    "    p = pathlib.Path(jpeg_path)\n",
    "    for target in target_list:\n",
    "        p_target = p.joinpath(str(target))\n",
    "        p_target.mkdir(exist_ok = True)\n",
    "    \n",
    "    l = list(p.glob('**/*.jpg'))\n",
    "    \n",
    "    for p_image in l:\n",
    "        image_name = p_image.stem\n",
    "        #print(image_name)\n",
    "        target = meta_df.loc[meta_df['image_name'] == image_name, 'target'].values[0]\n",
    "        #print(target)\n",
    "        p_target = p.joinpath(str(target)).joinpath(p_image.name)\n",
    "        shutil.move(p_image, p_target)\n",
    "        #print(p_target)\n",
    "        #break\n",
    "    print('Images moved to target subfolders')\n",
    "        \n",
    "#move_to_target_subfolders(meta, JPEG_TRAIN, [0, 1])\n",
    "#move_to_target_subfolders(meta, JPEG_VAL, [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = split_train_val(meta, JPEG_TRAIN, JPEG_VAL)\n",
    "meta"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_lookup = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=tf.constant(meta['image_name']),\n",
    "        values=tf.constant(meta['target']),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"target_lookup\"\n",
    ")\n",
    "\n",
    "# test lookup\n",
    "input_tensor = tf.constant(meta['image_name'][91])\n",
    "out = label_lookup.lookup(input_tensor)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess_images(img):\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, [HEIGHT, WIDTH])\n",
    "    return img\n",
    "\n",
    "def augment(img):\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "        \n",
    "    k = tf.random.uniform(shape = (), minval=0, maxval=4, dtype=tf.int32)\n",
    "    #tf.print(k)\n",
    "    img = tf.image.rot90(img, k)\n",
    "    return img\n",
    "    \n",
    "def process_path(jpeg_path):\n",
    "    image_name = tf.strings.split(tf.strings.split(jpeg_path, os.sep)[-1], '.')[0]\n",
    "    train_val_test = tf.strings.split(jpeg_path, os.sep)[3]\n",
    "\n",
    "    label = tf.strings.split(jpeg_path, os.sep)[-2]\n",
    "    label = tf.strings.to_number(label, tf.int32) \n",
    "    \n",
    "    img = tf.io.read_file(jpeg_path)\n",
    "    img = tf.io.decode_jpeg(img)\n",
    "    \n",
    "    if label == 0 or train_val_test == tf.constant('val', dtype = tf.string):\n",
    "        #tf.print('val condition')\n",
    "        img = preprocess_images(img)\n",
    "    else:\n",
    "        img = preprocess_images(img)\n",
    "        img = augment(img)\n",
    "    \n",
    "    return img, label\n",
    "\n",
    "# https://www.tensorflow.org/tutorials/load_data/images\n",
    "def prepare_for_training(ds, batch_size, cache=True, shuffle_buffer_size=100):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    if batch_size > 0:\n",
    "        ds = ds.batch(batch_size)\n",
    "    \n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "def dataset_from_jpeg(jpeg_path, batch_size, shuffle_buffer_size, prepare = False):\n",
    "    ds = tf.data.Dataset.list_files(str(jpeg_path + '*.jpg'))\n",
    "    ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    if prepare:\n",
    "        ds = prepare_for_training(ds, batch_size, cache = False, \n",
    "                                  shuffle_buffer_size = shuffle_buffer_size)\n",
    "    else:\n",
    "        #ds = ds.cache()\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "        ds = ds.repeat()\n",
    "    return ds\n",
    "\n",
    "def dataset_combine(jpeg_path, batch_size, cache = True, shuffle_buffer_size = 1000):\n",
    "    ds_0 = dataset_from_jpeg(str(jpeg_path + '/0'), False, shuffle_buffer_size, prepare = False)\n",
    "    ds_1 = dataset_from_jpeg(str(jpeg_path + '/1'), False, shuffle_buffer_size, prepare = False)\n",
    "\n",
    "    ds = tf.data.experimental.sample_from_datasets([ds_0, ds_1], weights=[0.5, 0.5])\n",
    "    ds = prepare_for_training(ds, batch_size, cache = False, shuffle_buffer_size = shuffle_buffer_size)\n",
    "    return ds\n",
    "\n",
    "ds_train = dataset_combine(JPEG_TRAIN, BATCH_SIZE, shuffle_buffer_size = 500)\n",
    "ds_val = dataset_from_jpeg(JPEG_VAL, BATCH_SIZE, shuffle_buffer_size = 150, prepare = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_process_path(jpeg_path):\n",
    "    img = tf.io.read_file(jpeg_path)\n",
    "    img = tf.io.decode_jpeg(img)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, [HEIGHT, WIDTH])\n",
    "    return img\n",
    "    \n",
    "def predict_dataset(jpeg_path):\n",
    "    ds = tf.data.Dataset.list_files(str(jpeg_path + '*.jpg'), shuffle = False)\n",
    "    ds = ds.map(predict_process_path, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "ds_test = predict_dataset(JPEG_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(image, label):\n",
    "    for i in range(BATCH_SIZE):\n",
    "        plt.figure()\n",
    "        plt.imshow(image[i,:,:,:])\n",
    "        plt.title(str(i) + ' - ' + str(label.numpy()[i]))\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for image, label in ds_train.take(4):\n",
    "    show(image, label)\n",
    "    print(label.shape)\n",
    "    print(image.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for image in ds_test.take(1):\n",
    "    show(image, tf.constant('1234'))\n",
    "    print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet50():\n",
    "    base_model = tf.keras.applications.ResNet50(include_top=True, weights=None, \n",
    "                                            input_tensor=None, input_shape=(HEIGHT, WIDTH, 3),\n",
    "                                            pooling=None)\n",
    "    #base_model.summary()\n",
    "    new_output = KL.Dense(1, activation = 'sigmoid', dtype = 'float32')(base_model.layers[-2].output)\n",
    "    model = Model(base_model.input, new_output)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=METRICS)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficientnet_b4():\n",
    "    base_model = tf.keras.applications.EfficientNetB4(\n",
    "        include_top=True, weights=None, input_tensor=None, input_shape=(HEIGHT, WIDTH, 3),\n",
    "        pooling=None, classes=2, classifier_activation='sigmoid')\n",
    "    bas_model.summary()\n",
    "    \n",
    "build_efficientnet_b4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = pathlib.Path('logs/siim-isic-melanoma-classification/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = 'models/siim-isic-melanoma-classification/resnet50-768-{epoch:02d}-{val_auc:.4f}.h5', \n",
    "    monitor = \"val_auc\",\n",
    "    mode='max',\n",
    "    save_best_only = True,\n",
    "    save_weights_only = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/siim-isic-melanoma-classification/resnet50-768-38-0.8852.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(ds_train, \n",
    "                    validation_data = ds_val,\n",
    "                    epochs = 20, \n",
    "                    steps_per_epoch = 400, \n",
    "                    validation_steps = 400, \n",
    "                    callbacks = [tensorboard_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/siim-isic-melanoma-classification/submit-04.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(ds_val, steps = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, jpeg_path, dataset, csv_filename, take = False):\n",
    "    p = pathlib.Path(jpeg_path)\n",
    "    l = list(p.glob('**/*.jpg'))\n",
    "    image_names = [p.stem for p in l]\n",
    "    \n",
    "    if not take:\n",
    "        print(take)\n",
    "        preds = model.predict(dataset)\n",
    "        preds = np.ndarray.flatten(preds)\n",
    "        df = pd.DataFrame({'image_name': image_names, \n",
    "                           'target': preds})\n",
    "    else:\n",
    "        print(take)\n",
    "        preds = model.predict(dataset.take(take))\n",
    "        preds = np.ndarray.flatten(preds)\n",
    "        df = pd.DataFrame({'image_name': image_names[:take*BATCH_SIZE], \n",
    "                           'target': preds})\n",
    "        \n",
    "    df.to_csv(pathlib.Path(SUBMITS_DIR, csv_filename), index = False)\n",
    "    return df\n",
    "\n",
    "predicts = predict(model, JPEG_TEST, ds_test, 'submit-05.csv', take = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
